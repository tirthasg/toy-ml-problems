{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NumPy, and few other libraries for utility functions\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a make_classification object to setup the Binary classification problem\n",
    "# Data-set size: 1 million, 50 dimensional instances, of which only 10 are informative\n",
    "# Other parameters: 5 redundant features (linear combinations of useful features), \n",
    "#                   weights = 0.7 (class-porportions),\n",
    "#                   class_sep = 0.5 (spread of cluster)\n",
    "\n",
    "X, y = make_classification(n_samples=1000000, n_features=50, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 50)\n",
      "(1000000,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of instances, and their corresponding labels\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a StandardScaler object to transform the data-set to mean equal to zero, and\n",
    "# standard deviation equal to 1.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.288745334539272e-16\n",
      "1.0000000000000016\n"
     ]
    }
   ],
   "source": [
    "# Printing out the mean, and standard deviation for the transform data-set.\n",
    "\n",
    "print(X.mean())\n",
    "print(X.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a train_test_split object to break the data set into 75% training data, and\n",
    "# 25% test data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 50)\n",
      "(750000,)\n",
      "(250000, 50)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "# Printing out the shapes of training, and testing instances, along with their corresponding labels.\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard implementation of the Sigmoid function\n",
    "# The Sigmoid function maps the value into the range [0, 1].\n",
    "\n",
    "def sigmoid(X_train, weights):\n",
    "    unbounded_predictions = np.dot(X_train, weights)\n",
    "    return 1 / (1 + np.exp(-unbounded_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Gradient Descent algorithm\n",
    "\n",
    "def gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "        \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "        \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    for _ in range(n_iters):\n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = sigmoid(X_train, weights)\n",
    "        \n",
    "        # Calculating the losses for every instance in the training data\n",
    "        losses = y_train - predictions\n",
    "        \n",
    "        # Calculating the gradient using the losses\n",
    "        gradients = np.dot(X_train.T, losses) / X_train.shape[0]\n",
    "                \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradients\n",
    "            \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions for the testing data using the estimated weights\n",
    "\n",
    "def make_prediction(X_test, weights):\n",
    "    # Appending an extra column of 1s into the testing data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    # Making a prediction using the estimated weights\n",
    "    predictions = sigmoid(X_test, weights)\n",
    "    \n",
    "    # Setting the threshold to 0.5 for class predictions\n",
    "    class_zero = (predictions < 0.50)  \n",
    "    \n",
    "    # Mapping values in the range [0, 1] to their corresponding classes\n",
    "    predictions[class_zero] = 0\n",
    "    predictions[~class_zero] = 1\n",
    "    \n",
    "    # Returing the predicted classes\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 7s, sys: 4min 17s, total: 9min 25s\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "0.724364\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Stochastic Gradient Descent algorithm\n",
    "\n",
    "def stochastic_gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "    \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    for _ in range(n_iters):\n",
    "        # Picking a random index in the valid index range\n",
    "        idx = np.random.randint(0, X_train.shape[0], size=1)\n",
    "        \n",
    "        # Picking the corresponding instance, and it's label\n",
    "        X_sampled = X_train[idx, :]\n",
    "        y_sampled = y_train[idx]\n",
    "        \n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = sigmoid(X_sampled, weights)\n",
    "        \n",
    "        # Calculating the loss for only the picked instance\n",
    "        loss = y_sampled - predictions\n",
    "        \n",
    "        # Calculating the gradient using the loss\n",
    "        gradient = np.dot(X_sampled.T, loss)\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradient \n",
    "    \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 237 ms, sys: 517 ms, total: 754 ms\n",
      "Wall time: 69.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Stochastic Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = stochastic_gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 1. 1.]\n",
      "0.70068\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Mini-batch Gradient Descent algorithm\n",
    "\n",
    "def mini_batch_gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "    \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    \n",
    "    # Setting the batch size to 1 / 100th the size of the training data\n",
    "    sample_size = X_train.shape[0] // 100\n",
    "    for _ in range(n_iters):\n",
    "        # Picking random indexes in the valid index range\n",
    "        idx = np.random.choice(X_train.shape[0], size=sample_size, replace=False)\n",
    "        \n",
    "        # Picking the corresponding instances, and their labels to create a batch\n",
    "        X_sampled = X_train[idx, :]\n",
    "        y_sampled = y_train[idx]\n",
    "        \n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = sigmoid(X_sampled, weights)\n",
    "        \n",
    "        # Calculating the losses for the batch\n",
    "        losses = y_sampled - predictions\n",
    "        \n",
    "        # Calculating the gradients using the losses\n",
    "        gradients = np.dot(X_sampled.T, losses) / X_sampled.shape[0]\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradients\n",
    "    \n",
    "    # Returning the estimated weights\n",
    "    return weights        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 3min 25s, total: 4min 48s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Mini-batch Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = mini_batch_gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.72354\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm | Training time | Accuracy Score         \n",
    "| :- |-------------: | :-:\n",
    "|Gradient Decent| 25.4 sec  | 0.7243\n",
    "| Stochastic Gradient Descent | 69.5 ms | 0.7006\n",
    "| Mini-Batch Gradient Descent | 12.6 sec | 0.7235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

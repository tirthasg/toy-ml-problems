{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NumPy, and few other libraries for utility functions\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a make_regression object to setup the Regression problem\n",
    "# Data-set size: 1 million, 50 dimensional instances, of which only 10 are informative\n",
    "# Other parameters: noise = 2 (The std of Gaussian noise added to the output)\n",
    "\n",
    "X, y = make_regression(n_samples=1000000, n_features=50, n_informative=10, noise=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 50)\n",
      "(1000000,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of instances, and their corresponding targets\n",
    "\n",
    "print(X.shape) \n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a StandardScaler object to transform the data-set to mean equal to zero, and\n",
    "# standard deviation equal to 1.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1253754312056116e-18\n",
      "0.9999999999999958\n"
     ]
    }
   ],
   "source": [
    "# Printing out the mean, and standard deviation for the transform data-set.\n",
    "\n",
    "print(X.mean())\n",
    "print(X.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a train_test_split object to break the data set into 75% training data, and\n",
    "# 25% test data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 50)\n",
      "(750000,)\n",
      "(250000, 50)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "# Printing out the shapes of training, and testing instances, along with their corresponding labels.\n",
    "\n",
    "print(X_train.shape) \n",
    "print(y_train.shape) \n",
    "print(X_test.shape) \n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Gradient Descent algorithm\n",
    "\n",
    "def gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "        \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    for _ in range(n_iters):\n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = np.dot(X_train, weights) \n",
    "        \n",
    "        # Calculating the losses for every instance in the training data\n",
    "        losses = y_train - predictions\n",
    "                \n",
    "        # Calculating the gradient using the losses\n",
    "        gradients = np.dot(X_train.T, losses) / X_train.shape[0]\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradients\n",
    "            \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions for the testing data using the estimated weights\n",
    "\n",
    "def make_prediction(X_test, weights):\n",
    "    # Appending an extra column of 1s into the testing data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    # Making a prediction using the estimated weights\n",
    "    predictions = np.dot(X_test, weights)\n",
    "    \n",
    "    # Returing the predicted targets\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 31s, sys: 2min 37s, total: 7min 8s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  68.07753259  -66.51451863   66.48432179 ... -213.02163516  -69.47504748\n",
      " -420.83644854]\n",
      "3.995328089614421\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Stochastic Gradient Descent algorithm\n",
    "\n",
    "def stochastic_gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "    \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    for _ in range(n_iters):\n",
    "        # Picking a random index in the valid index range\n",
    "        idx = np.random.randint(X_train.shape[0], size=1)\n",
    "        \n",
    "        # Picking the corresponding instance, and it's target\n",
    "        X_sampled = X_train[idx, :]\n",
    "        y_sampled = y_train[idx]\n",
    "        \n",
    "        # Making a prediction for the current value of weights\n",
    "        prediction = np.dot(X_sampled, weights)\n",
    "        \n",
    "        # Calculating the loss for only the picked instance\n",
    "        loss = y_sampled - prediction\n",
    "        \n",
    "        # Calculating the gradient using the loss\n",
    "        gradient = np.dot(X_sampled.T, loss)\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradient \n",
    "    \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.8 ms, sys: 32.5 ms, total: 73.3 ms\n",
      "Wall time: 72.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Stochastic Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = stochastic_gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  67.77391911  -66.50701263   67.56547764 ... -213.0578028   -67.44135778\n",
      " -420.40236108]\n",
      "5.040949667776625\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Mini-batch Gradient Descent algorithm\n",
    "\n",
    "def mini_batch_gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "    \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    \n",
    "    # Setting the batch size to 1 / 100th the size of the training data\n",
    "    sample_size = X_train.shape[0] // 100\n",
    "    for _ in range(n_iters):\n",
    "        # Picking random indexes in the valid index range\n",
    "        idx = np.random.choice(X_train.shape[0], size=sample_size, replace=False)\n",
    "\n",
    "        # Picking the corresponding instances, and their labels to create a batch\n",
    "        X_sampled = X_train[idx, :]\n",
    "        y_sampled = y_train[idx]\n",
    "        \n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = np.dot(X_sampled, weights)\n",
    "        \n",
    "        # Calculating the losses for the batch\n",
    "        losses = y_sampled - predictions\n",
    "        \n",
    "        # Calculating the gradients using the losses\n",
    "        gradients = np.dot(X_sampled.T, losses) / X_sampled.shape[0]\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradients\n",
    "     \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 3min 20s, total: 4min 41s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Mini-batch Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = mini_batch_gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  68.07786906  -66.52178991   66.49323149 ... -213.02064209  -69.48933778\n",
      " -420.84215135]\n",
      "3.9954672127309285\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm | Training time | MSE         \n",
    "| :- |-------------: | :-:\n",
    "|Gradient Decent| 18.7 sec  | 3.9953\n",
    "| Stochastic Gradient Descent | 72.9 ms | 5.0409\n",
    "| Mini-Batch Gradient Descent | 12.3 sec | 3.9954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
